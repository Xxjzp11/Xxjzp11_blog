---
id: 028 Python的爬虫三
title: 028 Python的爬虫三
author: Xxjzp11
author_title: studying
author_url: https://github.com/Xxjzp11
author_image_url: https://avatars2.githubusercontent.com/u/68409850?s=460&u=144d3c818e76fe4b88687db84279fad48b198818&v=4
tags: [python, notes]
---

# 028 Python的爬虫三

<!--truncate-->

------------

## 异常处理

### URLError

当urlopen()方法无法处理一个响应的时候，就会引发一个URLError的异常

通常在没有网络连接或者对方服务器压根不存在的情况下就会引发这个异常

同时这个URLError会伴随一个reason的属性，包含一个由错误编码和错误信息组成的元组

### HTTPError

是URLError的一个子类

服务器上每个http请求的响应都会返回一个数字状态码，例如404

状态码100 ~ 299的范围表示成功，400~599表示有问题

可参考：[[HTTP状态码|菜鸟教程]](https://www.runoob.com/http/http-status-codes.html)

### 处理异常的第一种写法

```python
from urllib.request import Request, urlopen
from urllib.error import URLError, HTTPError
req = Request(someurl)
try:
    response = urlopen(req)
except HTTPError as e:
    print('The server couldn\'t fulfill the request.')
    print('Error code: ', e.code)
except URLError as e:
    print('We failed to reach a server.')
    print('Reason: ', e.reason)
else:
# everything is fine

# HTTPError要写在URLError前
```

### 处理异常的第二种写法（建议）

```python
from urllib.request import Request, urlopen
from urllib.error import URLError
req = Request(someurl)
try:
    response = urlopen(req)
except URLError as e:
    if hasattr(e, 'reason'):
        print('We failed to reach a server.')
        print('Reason: ', e.reason)
    elif hasattr(e, 'code'):
        print('The server couldn\'t fulfill the request.')
        print('Error code: ', e.code)
else:
# everything is fine
```

--------------------

## Scrapy

- 说到Python爬虫，大牛们都会不约而同地提起Scrapy。因为Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。

- Scrapy最初是为了页面抓取（更确切来说, 网络抓取）所设计的，也可以应用在获取API所返回的数据（例如Amazon Associates Web Services）或者通用的网络爬虫。

使用Scrapy抓取一个网站一共需要四个步骤

- 创建一个Scrapy项目
- 定义Item容器
- 编写爬虫
- 存储内容

![028-01.png](https://www.cdnjson.com/images/2021/07/25/028-01.png)

首先我们来分析它的几大组件：

**Scrapy Engine**：它是 Scrapy 的核心，爬虫工作的核心。负责控制数据流在系统中所有组件之间的流动，大家可以看到，无论那两个组件之间进行交流，都必须经过它

**Downloader** ：下载器，下载器负责获取页面的数据，然后提供给 Spiders，数据是从 Scheduler（调度器）这里获得的

**Scheduler**：调度器，是从Scrapy Engine（引擎）这里接收 Requests 数据，事实上，Requests 数据需要的 request 的网页的地址是存放在 Spiders 这里，Spiders提供给Scrapy Engine ，Scrapy Engine（引擎）发送 Requests 给 Scheduler（调度器），调度器再把 Requests 传给 Downloader，Downloader 获得内容（也就是 Responses）之后，就发给Scrapy Engine，然后发给 Spiders 分析

那么 Spiders 就是 Scrapy 用户编写用于分析下载器返回回来的 Responses，然后提取出 Items 和 需要跟进 的url 的类

**Item Pipeline**，负责处理被 Spiders 提取出来的 Items，Items 就是一个容器，存放我们需要的内容的一个容器，它把 Items 进行存储化，例如存到数据库，存到文件，就是由 Item Pipeline 来处理的

**两个中间键**，两个中间件事实上就是提供一个简便的机制，通过让你插入自定义的代码来扩展 Scrapy 的功能

下载器中间件，Downloader Middlewares，是在引擎和下载器之间的特定钩子，是处理 Downloader 发到引擎的Responses，Responses 要发给 Spiders 需要经过 引擎，下载器中间件就在中间 hook 一下

Spiders 中间件，Spiders Middlewares，是处理Spiders 和引擎之间交互的 hook，首先它是接收来自 Downloader 的数据，接收Response 要先从Spiders中间件这里过滤一下，进行额外的操作，然后再给Spiders，然后呢，这个中间件也会接收spiders 的输出，例如 Requests和 Items

### 步骤一：创建一个Scrapy项目

在终端中输入命令：

```
scrapy startproject tutorial
```

这样就在当前工作目录下创建了一个tutorial文件夹，这就是一个Scrapy项目

这个文件夹就是按照下面的形式存储的

```
tutorial/
    scrapy.cfg
    tutorial/
        __init__.py
        items.py
        pipelines.py
        settings.py
        spiders/
            __init__.py
            ...
```

scrapy.cfg 是项目的配置文件（暂时不用，保持默认即可）

tutorial 子文件夹，存放的是模块的代码，也是我们要填充的代码

items.py 是项目中的容器

### 步骤二：定义Item容器

- Item是保存爬取到的数据的容器，其使用方法和python字典类似，并且提供了额外保护机制来避免拼写错误导致的未定义字段错误

我们想获取 标题 和 超链接 和 描述，只需要在 items.py文件里建立相应的字段：

初次打开未经修改的内容如下：

```python
# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy

class TutorialItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    pass
```

其中都已经注释好了，

> #name = scrapy.Field() ;
>
> name：就是你要建立的字段的名字
>
> scrapy.Field()：就是对应的占位符。
>

我们就照着写就可以了：

```python
class CurlieItem(scrapy.Item):  #改个与项目对应的名字
    # define the fields for your item here like:
    # name = scrapy.Field()
    title = scrapy.Field()   #标题
    link = scrapy.Field()   #超链接
    desc = scrapy.Field()  #描述
```

### 步骤三：编写爬虫

- 编写爬虫，我们就写在 spiders 文件夹里面，其实就是编写爬虫类Spider，Spider 是用户编写用于从网站上爬取数据的类

- 其包含一个用于下载的初始 URL，然后是如何跟进网页中的链接以及如何分析页面中的内容，还有提取生成 item 的方法

这就包含两个部分，第一个部分就是写一个初始化 URL ，例如 我们这里初始化 是从  https://curlie.org/Computers/Programming/Languages/Python/Books/  和  https://curlie.org/Computers/Programming/Languages/Python/Resources/  这两个 URL下载，我们就把它列到 spider 里面，然后就是还需要写一个方法，如何分析页面中的内容，还有生成 item 

我们的操作是：**在spider 里创建一个 Curlie_spider.py 的源文件**

**我们首先写一个 Spider 类，我们命名为 CurlieSpider，这里要求必须是继承 scray.Spider 类，首先需要有一个 name，name 这里必须是唯一的，用来确认你这只 蜘蛛 的名字**

**接着有一个 allowed_domains，是一个列表，确定这只蜘蛛要爬取的范围，这里我们规定只能爬取在 curlie.org 网址里面**。这样它只会在这个域名里面去爬

**接下来就是 start_urls ，这里是开始爬取的网址，规定从哪里开始爬**

**接下来写一个分析的方法，命名为 parse，有一个唯一的参数 response。**

事实上，我们看一下 Scrapy 的框架图，我们前面写的内容就是由 Scrapy Engine 从 Spiders 提取，然后变成 Requests 给 Schedulder，然后我们刚刚说了，downloader 会下载出来的 Reponses 数据给 Scrapy Engine ，然后给 Spiders，我们要一个分析机来处理，这就是我们的**parse方法，这个方法接收 Responses，然后对它进行分析处理，并且提取成 Items 给 Item Pipeline，所以我们就要在这个方法里写一些指定的代码**。

```python
# curlie_spider.py
import scrapy

class CurlieSpider(scrapy.Spider):
    name = 'curlie'
    allowed_domains = ['curlie.org']
    start_urls = [
        'https://curlie.org/Computers/Programming/Languages/Python/Books/',
        'https://curlie.org/Computers/Programming/Languages/Python/Resources/'
    ]

    def parse(self, response):
        filename = response.url.split('/')[-2]
        with open(filename, 'wb') as f:
            f.write(response.body)  # response.body就是这个网页的源代码
```

保存curlie_spider.py文件，我们把这个爬取分为先爬后取两个独立动作，展开给大家看：

#### 首先是爬

在 终端 中，目录切到 tutorial 根目录，调用命令 scrapy crawl curlie

- 这里的 crawl 翻译过来就是 爬取 的意思，curlie 就是我们选择的蜘蛛，我们在 curlie_spider 里写了一个 name 叫做 curlie，它就知道调用哪个爬虫去工作了

```
scrapy crawl curlie
```

爬取完成之后会发现 tutorial 根目录下多了 Books 和 Resources 两个文件

如果用编辑器打开的话，实际上就是上面那个网页的源代码（保存的是 response.body）

#### 爬完整个网页，接下来就是取的过程

我们现在的目标就是要从这个 Books 和 Resources 这个偌大的内容中找出 title 、link 和 desc ，然后分别保存提取出来，大家知道，这就是一个大浪淘沙的过程

#### Scrapy Selectors

**在Scrapy 里面，这是使用一种基于 XPath 和 CSS 的表达式机制**

Selectors 是一个选择器，它有4个基本方法：

- **xpath()**：传入 xpath 表达式，返回该表达式所对应的所有节点的 selector list 列表
- **css()**：传入 css 表达式，返回该表达式所对应的所有节点的 selector list 列表
- **extract()**：序列化该节点为 unicode 字符串并返回 list
- **re()**：根据传入的正则表达式对数据进行提取，返回 unicode 字符串 list 列表

#### Scrapy shell

在终端中进入项目的根目录（在前面我们已经进入了），输入

```
scrapy shell "网址"
```

这个直接返回的就是网址的HTML 代码，会**返回到response上**

所以我们就**可以直接使用 response**进行操作。

比如查看网页的 body、headers

```
response.body
response.headers
```

**selector 选择器就是这么一个筛子，正如我们刚才所讲到的，可以使用 response.selector.xpath() 或者 response.selector.css() 或者 response.selector.extract() 或者 response.selector. re() 这四个基本方法来进行筛选**

#### XPath()

**XPath 是一门在网页中查找特定信息的语言。所以用 Xpath 来筛选数据，比使用正则表达式容易些**

事实上，你使用正则表达式来查找 html 这类的网页文件的话，经常会出现一些问题，用 XPath 就不会，因为它是针对性的

我们这里给出一个 XPath 表达式的例子，以及对应的含义：

```
/html/head/title：选择HTML文档中<head>标签中的<title>元素</font>
/html/head/title/text()：选择上面提到的<title>元素的文字
//td：选择所有的 <td> 元素
//div[@class=“mine”]：选择所有具有 class="mine"属性的 div 元素
//a[@href]：选择a标签里的链接
```

在scrapy shell中使用

```ruby
>>> response.xpath('//title')
[<Selector xpath='//title' data='<title>Curlie - Computers: Programmin...'>]
# .extract()可以让筛选结果变成一个字符串
>>> response.xpath('//title').extract()
['<title>Curlie - Computers: Programmin\xadg: Languages: Python: Books</title>']
# '/text()'可以只得标签里的内容
>>> response.xpath('//title/text()').extract()
['Curlie - Computers: Programmin\xadg: Languages: Python: Books']
```

接下来就是提取数据了，尝试通过审查元素来得到我们想要的 title，link 和 desc 的规律

![028-02.png](https://www.cdnjson.com/images/2021/07/25/028-02.png)

观察后，我们发现了 title，link 和 desc 在标签中的位置

以title为例，我们发现了：

![028-03.png](https://www.cdnjson.com/images/2021/07/25/028-03.png)

发现，网页的标题栏也被取得了

然后我通过标签的 class 来筛选我们所要的 title

![028-04.png](https://www.cdnjson.com/images/2021/07/25/028-04.png)

修改 xpath 的查询规则，来获得我们想要的 title，link 和 desc 

```ruby
# 获取title
response.xpath('//div[@class="site-sort-by-date"]/div/div/div[@class="site-title"]/a/text()').extract()
# 获取link
response.xpath('//div[@class="site-sort-by-date"]/div/div/div[@class="site-title"]/a/@href').extract()
# 获取desc
response.xpath('//div[@class="site-sort-by-date"]/div/div/div[@class="site-descr"]/text()[1]').extract()
```

修改我们的 Spider 代码，也就是 curlie_spider.py。我们就按刚才从 shell 获得的经验来写 parse() 函数

```python
# curlie_spider.py
import scrapy

class CurlieSpider(scrapy.Spider):
    name = 'curlie'
    allowed_domains = ['curlie.org']
    start_urls = [
        'https://curlie.org/Computers/Programming/Languages/Python/Books/',
        'https://curlie.org/Computers/Programming/Languages/Python/Resources/'
    ]

    def parse(self, response):
        titles = response.selector.xpath('//div[@class="site-sort-by-date"]/div/div/div[@class="site-title"]/a/text()').extract()
        links = response.selector.response.xpath('//div[@class="site-sort-by-date"]/div/div/div[@class="site-title"]/a/@href').extract()
        descs = response.selector.response.xpath('//div[@class="site-sort-by-date"]/div/div/div[@class="site-descr"]/text()[1]').extract()
        if len(titles) == len(links) == len(descs):
            for i in range(len(titles)):
                print(titles[i], links[i], descs[i])
```

写好之后，保存，进入 终端，在 tutorial 根目录下执行命令：

```
scrapy crawl curlie
```

### 步骤四：存储内容

我们希望 Spider 将爬取然后筛选后的数据存放到 Items 容器里面，我们刚才也在 parse 里写了筛选出 Items 对应的数据的方法了。筛选之后，我希望将它存放到 Items 中去u

items 既是容器，也是一个类，类名我们在这个项目中定义CurlieItem

我们需要 把 items 导入到 spider 中，才可以使用它，于是，我们在 curlie_spider.py 文件中引入

```
from tutorial.items import CurlieItem
```

```python
# curlie_spider.py
import scrapy
from tutorial.items import CurlieItem

class CurlieSpider(scrapy.Spider):
    name = 'curlie'
    allowed_domains = ['curlie.org']
    start_urls = [
        'https://curlie.org/Computers/Programming/Languages/Python/Books/',
        'https://curlie.org/Computers/Programming/Languages/Python/Resources/'
    ]

    def parse(self, response):
        titles = response.selector.xpath('//div[@class="site-sort-by-date"]/div/div/div[@class="site-title"]/a/text()').extract()
        links = response.selector.response.xpath('//div[@class="site-sort-by-date"]/div/div/div[@class="site-title"]/a/@href').extract()
        descs = response.selector.response.xpath('//div[@class="site-sort-by-date"]/div/div/div[@class="site-descr"]/text()[1]').extract()
        items = []
        if len(titles) == len(links) == len(descs):
            for i in range(len(titles)):
                # print(titles[i], links[i], descs[i])
                item = CurlieItem()
                item['title'] = titles[i]
                item['link'] = links[i]
                item['desc'] = descs[i]
                items.append(item)
            return items
```

然后我们在终端中，tutorail 的根目录下，执行命令：

```
scrapy crawl curlie -o items.json -t json
# -o 文件名 -t 保存形式
# 保存类型有json, jsonlines, csv, xml
```

执行完毕后，在 tutorial 根目录 下就会有一个名为 items.json 的文件