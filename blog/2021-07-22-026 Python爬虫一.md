---
id: 026 Python爬虫一
title: 026 Python爬虫一
author: Xxjzp11
author_title: studying
author_url: https://github.com/Xxjzp11
author_image_url: https://avatars2.githubusercontent.com/u/68409850?s=460&u=144d3c818e76fe4b88687db84279fad48b198818&v=4
tags: [python, notes]
---

# 026 Python爬虫一

<!--truncate-->

---------

## URL由三部分组成

URL的一般格式为（带方括号[]的为可选项）：

protocol :// hostname[:port] / path / \[;parameters][?query]#fragment

- 协议（不可缺少）：http、https、ftp......
- 存放资源的服务器的域名系统（DNS）主机名或IP地址（有时候需要端口号）（不可缺少）
- 主机资源的具体地址，如目录或文件名等（可以省略）

--------------

## urllib模块

Python2中urllib还分urllib和urllib2，到了Python3就只有urllib模块，**这其实也不是一个模块，它是一个包（package）**。

![026-01.png](https://www.cdnjson.com/images/2021/07/22/026-01.png)

总共有4个模块，第一个模块`urllib.request`是最重要的模块，它包含了对服务器请求的发出、跳转、代理和安全等各个方面。

```python
>>> import urllib.request
>>> response = urllib.request.urlopen('http://www.fishc.com')
>>> html = response.read()
>>> print(html)


b'<!DOCTYPE html>\n<html lang="en">\n<head>\n<meta charset="UTF-8">\n<meta name="apple-mobile-web-app-capable" content="yes">\n<meta name="apple-touch-fullscreen" content="yes">\n<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">\n<meta name="keywords" content="\xe9\xb1\xbcC\xe5\xb7\xa5\xe4\xbd\x9c\xe5\xae\xa4|\xe5\x85\x8d\xe8\xb4\xb9\xe7\xbc\x96\xe7\xa8\x8b\xe8\xa7\x86\xe9\xa2\x91\xe6\x95\x99\xe5\xad\xa6|Pyth
......
......
s[1] : 0;\n                // \xe6\xa0\xb9\xe6\x8d\xae\xe5\x85\xb3\xe7\xb3\xbb\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\xa4\xe6\x96\xad\n                if (Sys.ie) alert(\'\xe8\xaf\xb7\xe4\xbd\xbf\xe7\x94\xa8\xe9\x9d\x9eIE\xe6\xb5\x8f\xe8\xa7\x88\xe5\x99\xa8\xe6\x89\x93\xe5\xbc\x80\xe6\x9c\xac\xe4\xb8\xbb\xe9\xa1\xb5\');\n\n            }\n            getExplore();\n        </script>\n<script type="text/javascript" src="build/js/storyjs-embed.js"></script>\n<div class="myICP">\n<a href="http://beian.miit.gov.cn/state/outPortal/loginPortal.action" target="_blank">\xe7\xb2\xa4ICP\xe5\xa4\x8718085999\xe5\x8f\xb7-2</a>\n</div>\n</body>\n</html>'

>>> html = html.decode('utf-8')
>>> print(html)


<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-touch-fullscreen" content="yes">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">
<meta name="keywords" content="鱼C工作室|免费编程视频教学|Python教学|Web开发教学|全栈开发教学|C语言教学|汇编教学|Win32开发|加密与解密|Linux教学">
<meta name="description" content="鱼C工作室为大家提供最有趣的编程视频教学。">
.....
.....

                                    (s = ua.match(/chrome\/([\d\.]+)/)) ? Sys.chrome = s[1] :
                                        (s = ua.match(/version\/([\d\.]+).*safari/)) ? Sys.safari = s[1] : 0;
                // 根据关系进行判断
                if (Sys.ie) alert('请使用非IE浏览器打开本主页');

            }
            getExplore();
        </script>
<script type="text/javascript" src="build/js/storyjs-embed.js"></script>
<div class="myICP">
<a href="http://beian.miit.gov.cn/state/outPortal/loginPortal.action" target="_blank">粤ICP备18085999号-2</a>
</div>
</body>
</html>
```

我们要对拿到的字节对象进行解码，把他变成Unicode编码，这样我们才能看懂。

--------------

## 爬虫实战

### 第一个例子是做一个最简单的爬虫

我们去网站上下载一张图片

```dart
import urllib.request

#方法一
response = urllib.request.urlopen("http://placekitten.com/g/200/300")
cat_img = response.read()
with open('cat_200_300.jpg', 'wb') as f:
    f.write(cat_img)

#方法二          
req = urllib.request.Request("http://placekitten.com/g/200/400")
response2 = urllib.request.urlopen(req)
cat_img2 = response2.read()
with open('cat_200_400.jpg', 'wb') as f:
    f.write(cat_img2)
```

两张方法皆可

首先，**urlopen的url参数既可以是一个字符串也可以是Request对象，如果你传入一个字符串，那么Python是会默认先帮你把目标字符串转换成Request对象，然后再传给urlopen函数**

然后，urlopen实际上返回的是一个类文件对象，因此你可以用read()方法来读取内容。除此之外，文档还告诉你有三个函数可能以后会用到：

- **geturl()：返回请求的url**
- **info()：返回一个httplib.HTTPMessage对象，包含远程服务器返回的头信息**
- **getcode()：返回HTTP状态码（200表示正常）**

```swift
import urllib.request

response = urllib.request.urlopen("http://placekitten.com/g/200/500")
print(response.geturl())
print(response.info())
print(response.getcode())
```

输出：

```dart
http://placekitten.com/g/200/500
Date: Tue, 28 Aug 2018 05:28:35 GMT
Content-Type: image/jpeg
Transfer-Encoding: chunked
Connection: close
Set-Cookie: __cfduid=d4759915be0d0d369ceb59fdfd4dcfcf71535434115; expires=Wed, 28-Aug-19 05:28:35 GMT; path=/; domain=.placekitten.com; HttpOnly
Access-Control-Allow-Origin: *
Cache-Control: public, max-age=86400
Expires: Wed, 29 Aug 2018 05:28:35 GMT
CF-Cache-Status: HIT
Vary: Accept-Encoding
Server: cloudflare
CF-RAY: 45146018823196a0-FRA


200  
```

### 第二个例子是用有道来翻译文本

首先打开官网[http://fanyi.youdao.com](http://fanyi.youdao.com/)

F12（或alt + command + i），切换到Network窗

![026-02.png](https://www.cdnjson.com/images/2021/07/22/026-02.png)

点击Method是POST的请求

![026-03.png](https://www.cdnjson.com/images/2021/07/22/026-03.png)

如果看不到Method

![026-04.png](https://www.cdnjson.com/images/2021/07/22/026-04.png)

HTTP是基于请求-响应的模式的，客户端发出的请求叫Request，服务端的响应叫Response。

Request Headers是客户端发送请求的Headers，这个常常被服务器用来判断是否来自“非人类”的访问。例如写个Python代码，然后利用这个代码批量访问网站的数据，这样服务器的压力就会增大，所以一般服务器不欢迎“非人类”的访问。

一般是通过这个User-Agent来识别，普通浏览器会通过该内容向访问网站提供你所使用的浏览器类型、操作系统、浏览器内核等信息的标识：
 `User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36`

而使用Python访问的话，User-Agent会被定义为Python-urllib/3.4。

但事实上，User-Agent可以自定义的。

Form Data就是POST提交的内容

![026-05.png](https://www.cdnjson.com/images/2021/07/22/026-05.png)

那么如何用Python提交POST表单呢

**urlopen函数有一个data参数，如果给这个参数赋值，那么HTTP的请求就是使用POST方式；如果data=None，也就是默认值，那么HTTP的请求就是使用GET方式**

![026-06.png](https://www.cdnjson.com/images/2021/07/22/026-06.png)

这里还告诉我们，这个data参数的值必须符合这个application/x-www-form-urlencoded的格式，然后要用urllib.parse.urlencode()将字符串转换为这个格式

```python
import urllib.request
import urllib.parse

url = 'https://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule'
data = {}
data['i'] = '你好'
data['from'] = 'AUTO'
data['to'] = 'AUTO'
data['smartresult'] = 'dict'
data['client'] = 'fanyideskweb'
data['salt'] = '16267608025076'
data['sign'] = 'fca0e2e6b05fcb01172ca17677f4ad1f'
data['lts'] = '1626760802507'
data['bv'] = '24ecb70ba6203e4453baed50aa26b78e'
data['doctype'] = 'json'
data['version'] = '2.1'
data['keyfrom'] = 'fanyi.web'
data['action'] = 'FY_BY_CLICKBUTTION'
data = urllib.parse.urlencode(data).encode('utf-8')
response = urllib.request.urlopen(url, data)
html = response.read().decode('utf-8')
print(html)
```

输出：

```json
{"type":"ZH_CN2EN","errorCode":0,"elapsedTime":0,"translateResult":[[{"src":"你好","tgt":"hello"}]]}
```

**字符串在Python3内部的表示是Unicode编码，因此，在做编码转换时，通常需要以Unicode作为中间编码，即先将返回的bytes对象的数据解码（decode）成Unicode，再从Unicode编码（encode）成另一种编码。**

有关编码的问题可以参考https://fishc.com.cn/thread-56452-1-1.html。

接下来是解析上面这个JSON格式的字符串：

```python
import urllib.request
import urllib.parse
import json

content = input('请输入需要翻译的内容：')

url = 'https://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule'
data = {}
data['i'] = content
data['from'] = 'AUTO'
data['to'] = 'AUTO'
data['smartresult'] = 'dict'
data['client'] = 'fanyideskweb'
data['salt'] = '16267608025076'
data['sign'] = 'fca0e2e6b05fcb01172ca17677f4ad1f'
data['lts'] = '1626760802507'
data['bv'] = '24ecb70ba6203e4453baed50aa26b78e'
data['doctype'] = 'json'
data['version'] = '2.1'
data['keyfrom'] = 'fanyi.web'
data['action'] = 'FY_BY_CLICKBUTTION'
data = urllib.parse.urlencode(data).encode('utf-8')     # 转换格式并编码为utf-8

response = urllib.request.urlopen(url, data)    # post请求必须带上data参数
html = response.read().decode('utf-8')

target = json.loads(html)

print('翻译结果：%s' % target['translateResult'][0][0]['tgt'])   # 一层一层剥掉
```

输出：

```go
请输入需要翻译的内容：I want to go home.
翻译结果：我想回家了。
```

遇到 {'errorCode': 50} 怎么办？

- 相信很多人玩小甲鱼的这个代码的时候都遇到了这个问题
- 注意有道的反爬虫机制
- https://blog.csdn.net/nunchakushuang/article/details/75294947
- 此处，删除了Request URL的 `translate_o?` 中的 `_o`

---------------

## 修改headers

下图是urllib.request.Request部分有关于设置User-Agent的叙述。

![026-07.png](https://www.cdnjson.com/images/2021/07/22/026-07.png)

### 设置这个headers参数有两种办法

**1. 实例化Request参数时将headers参数传进去**

```python
import urllib.request
import urllib.parse
import json

content = input('请输入需要翻译的内容：')

url = 'https://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule'
head = {}
head['Referer'] = 'http://fanyi.youdao.com/'
head['User-Agent'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'

data = {}
data['i'] = content
data['from'] = 'AUTO'
data['to'] = 'AUTO'
data['smartresult'] = 'dict'
data['client'] = 'fanyideskweb'
data['salt'] = '16267608025076'
data['sign'] = 'fca0e2e6b05fcb01172ca17677f4ad1f'
data['lts'] = '1626760802507'
data['bv'] = '24ecb70ba6203e4453baed50aa26b78e'
data['doctype'] = 'json'
data['version'] = '2.1'
data['keyfrom'] = 'fanyi.web'
data['action'] = 'FY_BY_CLICKBUTTION'
data = urllib.parse.urlencode(data).encode('utf-8')

req = urllib.request.Request(url, data, head)
response = urllib.request.urlopen(req)
html = response.read().decode('utf-8')

target = json.loads(html)
print('翻译结果：%s' % target['translateResult'][0][0]['tgt'])
print(req.headers)
```

输出：

```python
请输入需要翻译的内容：你好
翻译结果：hello
{'Referer': 'http://fanyi.youdao.com/', 'User-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'}
```

**2. 通过add_header()方法往Request对象添加headers**

```kotlin
import urllib.request
import urllib.parse
import json

content = input('请输入需要翻译的内容：')

url = 'https://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule'

data = {}
data['i'] = content
data['from'] = 'AUTO'
data['to'] = 'AUTO'
data['smartresult'] = 'dict'
data['client'] = 'fanyideskweb'
data['salt'] = '16267608025076'
data['sign'] = 'fca0e2e6b05fcb01172ca17677f4ad1f'
data['lts'] = '1626760802507'
data['bv'] = '24ecb70ba6203e4453baed50aa26b78e'
data['doctype'] = 'json'
data['version'] = '2.1'
data['keyfrom'] = 'fanyi.web'
data['action'] = 'FY_BY_CLICKBUTTION'
data = urllib.parse.urlencode(data).encode('utf-8')

req = urllib.request.Request(url, data)
req.add_header('Referer', 'http://fanyi.youdao.com')
req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36')
response = urllib.request.urlopen(req)
html = response.read().decode('utf-8')

target = json.loads(html)
print('翻译结果：%s' % target['translateResult'][0][0]['tgt'])
print(req.headers)
```

输出：

```go
请输入需要翻译的内容：你好
翻译结果：hello
{'Referer': 'http://fanyi.youdao.com/', 'User-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'}
```

------------------

## 爬虫被拒问题

如果同一个IP地址在短时间内对服务器访问很频繁，显然这不是正常人操作的，所以服务器会设置每个IP的访问频率，一旦超过这个阙值，便认为就是爬虫，于是返回一个验证码页面，要求用户填写验证码，爬虫自然就不能正常爬取信息了，所以被拒绝。所以想到有2种方法可以解决这种情况：

### 延迟提交的时间

```python
import urllib.request
import urllib.parse
import json
import time

while True:
    content = input('请输入需要翻译的内容(输入"q!"退出程序)：')
    if content == 'q!':
        break

    url = 'https://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule'

    data = {}
    data['i'] = content
    data['from'] = 'AUTO'
    data['to'] = 'AUTO'
    data['smartresult'] = 'dict'
    data['client'] = 'fanyideskweb'
    data['salt'] = '16267608025076'
    data['sign'] = 'fca0e2e6b05fcb01172ca17677f4ad1f'
    data['lts'] = '1626760802507'
    data['bv'] = '24ecb70ba6203e4453baed50aa26b78e'
    data['doctype'] = 'json'
    data['version'] = '2.1'
    data['keyfrom'] = 'fanyi.web'
    data['action'] = 'FY_BY_CLICKBUTTION'
    data = urllib.parse.urlencode(data).encode('utf-8')

    req = urllib.request.Request(url, data)
    req.add_header('Referer', 'http://fanyi.youdao.com')
    req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36')
    response = urllib.request.urlopen(req)
    html = response.read().decode('utf-8')

    target = json.loads(html)
    print('翻译结果：%s' % target['translateResult'][0][0]['tgt'])
    time.sleep(5) #停5秒
```

### 使用代理

步骤：

1.参数是一个字典 { ‘类型’ : ‘代理ip : 端口号’ }

- proxy_support = urllib.request.ProxyHandler({})

2.定制、创建一个 opener

- opener = urllib.request.build_opener(proxy_support)

3a.安装 opener

- urllib.request.install_opener(opener)

3b.调用 opener

- opener.open(url)

例子：

```python
import urllib.request

url = 'http://www.whatismyip.com.tw/'
proxy_support = urllib.request.ProxyHandler({'http' : '221.224.136.211:35101'})
# 接着创建一个包含代理IP的opener
opener = urllib.request.build_opener(proxy_support)
# 修改heades（可选）
opener.addheaders = [('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36')]
# 安装进默认环境
urllib.request.install_opener(opener)
# 试试看IP地址改了没
response = urllib.request.urlopen(url)
html = response.read().decode('utf-8')
print(html)
```

```python
import urllib.request
import random

url = 'http://www.whatismyip.com.tw/'
print("添加代理IP地址（IP：端口号），多个IP地址间用英文的分号隔开！")
iplist = input("请开始输入：").split(sep=";")
while True:
    ip = random.choice(iplist)
    proxy_support = urllib.request.ProxyHandler({'http':ip})
    opener = urllib.request.build_opener(proxy_support)
    opener.addheaders = [('User-Agent',
                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36')]
    urllib.request.install_opener(opener)
    try:
        print("正在尝试使用%s访问..." % ip)
        response = urllib.request.urlopen(url)
    except urllib.error.URLError:
        print("访问出错！")
    else:
        print("访问成功！")
    if input("请问是否继续？（Y/N）") == 'N':
        break
```

--------

## Beautiful Soup 4

Beautiful Soup 是一个可以从 HTML 或 XML 文件中提取数据的 Python 库。它能够通过你喜欢的转换器实现惯用的文档导航，查找，修改文档的方式。Beautiful Soup 会帮你节省数小时甚至数天的工作时间

官方地址：https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/index.html

------------

## 对煎蛋网的爬虫

```python
import urllib.request
import os
import base64
import datetime

def urlopen(url):
    req = urllib.request.Request(url)
    req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36')
    response = urllib.request.urlopen(req)
    html = response.read()
    return html

def get_page(url):
    html = urlopen(url).decode('utf-8')
    a = html.find('current-comment-page') + 23
    b = html.find(']', a)
    return html[a:b]

def find_imgs(url):
    html = urlopen(url).decode('utf-8')
    img_addrs = []
    a = html.find('img src=')
    while a != -1:
        b = html.find('.jpg', a, a+255)
        if b != -1:
            img_addrs.append('http:%s' % html[a+9:b+4])
        else:
            b = a + 9
        a = html.find('img src=', b)
    return img_addrs


def save_imgs(floder, img_addrs):
    for each in img_addrs:
        filename = each.split('/')[-1]
        with open(filename, 'wb') as f:
            img = urlopen(each)
            f.write(img)

def page_encode(page_num):
    today = datetime.date.today()
    page = '%d%02d%d-%d' % (today.year, today.month, today.day,page_num)
    str1 = base64.b64encode(page.encode('utf-8')).decode('utf-8') 
    return str1

def download_mm(floder='OOXX', pages=10):
    os.mkdir(floder)
    os.chdir(floder)
    
    url = 'http://jandan.net/ooxx/'
    page_num = int(get_page(url))


    for i in range(pages):
        page_num -= i
        aa = page_encode(page_num)
        page_url = url + aa + '#comments'
        img_addrs = find_imgs(page_url)
        save_imgs(floder, img_addrs)

if __name__ == '__main__':
    download_mm()
```

----------

## 测试题一

**0. 请问 URL 是“统一资源标识符”还是“统一资源定位符”？**

往后的学习你可能会经常接触 URL 和 URI，为了防止你突然懵倒，所以在这里给大家简单普及下。**URI是统一资源标识符（Universal Resource Identifier），URL 是统一资源定位符（Universal Resource Locator）。**用一句话概括它们的区别：URI 是用字符串来标识某一互联网资源，而 URL 则是表示资源的地址（我们说某个网站的网址就是 URL），因此 URI 属于父类，而 URL 属于 URI 的子类。



**1. 什么是爬虫？**

爬虫事实上就是一个程序，用于沿着互联网结点爬行，不断访问不同的网站，以便获取它所需要的资源。



**2. 设想一下，如果你是负责开发百度蜘蛛的攻城狮，你在设计爬虫时应该特别注意什么问题？**

不要重复爬取同一个 URL 的内容。假设你没做这方面的预防，如果一个 URL 的内容中包含该 URL 本身，那么就会陷入无限递归



**3. 设想一下，如果你是网站的开发者，你应该如何禁止百度爬虫访问你网站中的敏感内容？（课堂上没讲，可以自行百度答案）**

在网站的根目录下创建并编辑 robots.txt 文件，用于表明您不希望搜索引擎抓取工具访问您网站上的哪些内容。此文件使用的是 Robots 排除标准，该标准是一项协议，所有正规搜索引擎的蜘蛛均会遵循该协议爬取。既然是协议，那就是需要大家自觉尊重，所以该协议一般对非法爬虫无效



**4. urllib.request.urlopen() 返回的是什么类型的数据？**

返回的是一个**HTTPResponse的实例对象**，它属于http.client模块

```rupy
>>> response = urllib.request.urlopen("http://www.fishc.com")
>>> type(response)
<class 'http.client.HTTPResponse'>
```

调用其read()方法才能读出URL的内容



**5. 如果访问的网址不存在，会产生哪类异常？（虽然课堂没讲过，但你可以动手试试）**

HTTPError



**6. 鱼C工作室（[http://www.fishc.com](http://www.fishc.com/)）的主页采用什么编码传输的？**

UTF-8 编码



**7. 为了解决 ASCII 编码的不足，什么编码应运而生？**

Unicode 编码。扩展阅读关于编码的那篇文章太长了，有鱼油说太生涩难懂，对于对编码问题还一头雾水的鱼油请看 -> [什么是编码？](http://bbs.fishc.com/thread-66084-1-1.html)



**8. 下载鱼C工作室首页（[http://www.fishc.com](http://www.fishc.com/)），并打印前三百个字节**

```python
>>> import urllib.request
>>> response = urllib.request.urlopen('http://www.fishc.com')
>>> print(response.read(300))
b'<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"\r\n\t"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">\r\n\r\n<!-- \r\n(c) 2011 \xc4\xbdubom\xc3\xadr Krupa, CC BY-ND 3.0\r\n -->\t\r\n\r\n<html xmlns="http://www.w3.org/1999/xhtml">\r\n\t<head>\r\n\t\t<meta http-equiv="content-type" content="text/html; charset=utf-8" />\r\n\t\t'
```



**9. 写一个程序，检测指定 URL 的编码**

演示：

![026-08.png](https://www.cdnjson.com/images/2021/07/22/026-08.png)

提示：

提供个“电池”给你用 -> [一次性解决你所有的编码检测问题](http://bbs.fishc.com/thread-66086-1-1.html)

```python
import urllib.request
import chardet

def main():
    url = input("请输入URL：")

    response = urllib.request.urlopen(url)
    html = response.read()

    # 识别网页编码
    encode = chardet.detect(html)['encoding']
    if encode == 'GB2312':
        encode = 'GBK'

    print("该网页使用的编码是：%s" % encode)
        
if __name__ == "__main__":
    main()
```

--------------

## 测试题二

**0. urlopen() 方法的 timeout 参数用于设置什么？**

timeout 参数用于设置连接的超时时间，单位是秒



**1. 如何从 urlopen() 返回的对象中获取 HTTP 状态码？**

```python
response = urllib.request.urlopen(url)
code = response.getcode()
```



**2. 在客户端和服务器之间进行请求-响应时，最常用的是哪两种方法？**

GET 和 POST



**3. HTTP 是基于请求-响应的模式，那是客户端发出请求，服务端做出响应；还是服务端发出请求，客户端做出响应呢？**

发出请求的永远是客户端，做出响应的永远是服务端



**4. User-Agent 属性通常是记录什么信息？**

普通浏览器会通过该内容向访问网站提供你所使用的浏览器类型、操作系统、浏览器内核等信息的标识



**5. 如何通过 urlopen() 使用 POST 方法像服务端发出请求？**

**urlopen 函数有一个 data 参数，如果给这个参数赋值，那么 HTTP 的请求就是使用 POST 方式；如果 data 的值是 NULL，也就是默认值，那么 HTTP 的请求就是使用 GET 方式**



**6. 使用字符串的什么方法将其它编码转换为 Unicode 编码？**

decode()。decode() 的作用是将其他编码的字符串转换成 unicode 编码，相反，encode() 的作用是将 unicode 编码转换成其他编码的字符串



**7. JSON 是什么鬼？**

JSON 是一种轻量级的数据交换格式，说白了这里就是用字符串把 Python 的数据结构封装起来，便与存储和使用



**8. 配合 EasyGui，给“下载一只猫“的代码增加互动：**

- 让用户输入尺寸；
- 如果用户不输入尺寸，那么按默认宽400，高600下载喵；
- 让用户指定保存位置。

程序实现如下图：

![026-09.png](https://www.cdnjson.com/images/2021/07/22/026-09.png)

![026-10.png](https://www.cdnjson.com/images/2021/07/22/026-10.png)

![026-11.jpg](https://www.cdnjson.com/images/2021/07/22/026-11.jpg)

```python
import easygui as g
import urllib.request

title = '下载一只喵'
msg = '请填写喵的尺寸'
fieldNames = ['宽：', '高：']
fieldValues = []
size = width, height = 400, 600
fieldValues = g.multenterbox(msg, title, fieldNames, size)

while 1:
    if fieldValues is None: break
    errmsg = ""
    try:
        width = int(fieldValues[0].strip())
    except:
        errmsg += '宽度必须为整数！'
    try:
        width = int(fieldValues[1].strip())
    except:
        errmsg += '高度必须为整数！'
    if errmsg == "":
        break
    fieldValues = g.multenterbox(errmsg, title, fieldNames, fieldValues)

url = "http://placekitten.com/%d/%d" % (width, height)

response = urllib.request.urlopen(url)
cat_img = response.read()

filepath = g.diropenbox('请选择存放喵的文件夹')

if filepath:
    filename = '%s/cat_%d_%d.jpg' % (filepath, width, height)
else:
    filename = 'cat_%d_%d.jpg' % (width, height)

with open(filename, 'wb') as f:
    f.write(cat_img)

if __name__ == '__mian__':
    main()
```

----------------

## 测试题三

**0. 服务器是如何识访问来自浏览器还是非浏览器的？**

通过发送的 HTTP 头中的 User-Agent 来进行识别浏览器与非浏览器，服务器还以 User-Agent 来区分各个浏览器



**1. 明明代码跟视频中的例子一样，一运行却出错了，但在不修改代码的情况下再次尝试运行却又变好了，这是为什么呢？**

在网络信息的传输中会出现偶然的“丢包”现象，有可能是你发送的请求服务器没收到，也有可能是服务器响应的信息不能完整送回来……尤其在网络阻塞的时候。所以，在设计一个“称职”的爬虫时，需要考虑到这偶尔的“丢包”现象



**2. Request 是由客户端发出还是由服务端发出？**

我们之前说 HTTP 是基于“请求-响应”模式，Request 即请求的意思，而 Response 则是响应的意思。由客户端首先发出 Request，服务器收到后返回 Response



**3. 请问如何为一个 Request 对象动态的添加 headers？**

add_header() 方法往 Request 对象添加 headers



**4. 简单来说，代理服务器是如何工作的？他有时为何不工作了？**

将信息传给代理服务器，代理服务器替你向你要访问的服务器发送请求，然后在将服务器返回的内容返回给你

因为有“丢包”现象发生，所以多了一个中间人就意味着会多一层发生“丢包”的几率，且大多数代理并不只为一个人服务，尤其是免费代理

PS：大家想做“坏坏”的事情时可以考虑多几层代理，一般来说路由器日志并不会保存很长时间，几层代理后，基本很难查到是谁请求的


**5. HTTP 有好几种方法（GET，POST，PUT，HEAD，DELETE，OPTIONS，CONNECT），请问你如何晓得 Python 是使用哪种方法访问服务器呢？**

使用 get_method() 方法获取 Request 对象具体使用哪种方法访问服务器。最常用的无非就是 GET 和 POST 了，**当 Request 的 data 参数被赋值的时候，get_method() 返回 'POST'，否则一般情况下返回 'GET'**



**6. 上一节课后题中有涉及到登陆问题，辣么，你还记得服务器是通过什么来确定你是登陆还是没登陆的么？他会持续到什么时候呢？**

是 cookie，服务器通过判断你提交的 cookie 来确定访问是否来自”熟人“。

简单来说 cookie 可以分成两类：

- 一类是即时过期的 cookies，称为“会话” cookies，当浏览器关闭时（这里是 python 的请求程序）自动清除
- 另一类是有期限的 cookies，由浏览器进行存储，并在下一次请求该网站时自动附带（如果没过期或清理的话）